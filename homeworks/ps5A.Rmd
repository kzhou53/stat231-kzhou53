---
title: "STAT 231: Problem Set 5A"
author: "Kim Zhou"
date: "due by 10 PM on Monday, March 22"
output: pdf_document
---

In order to most effectively digest the textbook chapter readings -- and the  new R commands each presents -- series A homework assignments are designed to encourage you to read the textbook chapters actively and in line with the textbook's Prop Tip of page 33:

"\textbf{Pro Tip}: If you want to learn how to use a particular command, we highly recommend running the example code on your own" 

A more thorough reading and light practice of the textbook chapter prior to class allows us to dive quicker and deeper into the topics and commands during class.  Furthermore, learning a programming lanugage is like learning any other language -- practice, practice, practice is the key to fluency.  By having two assignments each week, I hope to encourage practice throughout the week.  A little coding each day will take you a long way!

*Series A assignments are intended to be completed individually.*  While most of our work in this class will be collaborative, it is important each individual completes the active readings.  The problems should be straightforward based on the textbook readings, but if you have any questions, feel free to ask me!

Steps to proceed:

\begin{enumerate}
\item In RStudio, go to File > Open Project, navigate to the folder with the course-content repo, select the course-content project (course-content.Rproj), and click "Open" 
\item Pull the course-content repo (e.g. using the blue-ish down arrow in the Git tab in upper right window)
\item Copy ps5A.Rmd from the course repo to your repo (see page 6 of the GitHub Classroom Guide for Stat231 if needed)
\item Close the course-content repo project in RStudio
\item Open YOUR repo project in RStudio
\item In the ps5A.Rmd file in YOUR repo, replace "YOUR NAME HERE" with your name
\item Add in your responses, committing and pushing to YOUR repo in appropriate places along the way
\item Run "Knit PDF" 
\item Upload the pdf to Gradescope.  Don't forget to select which of your pages are associated with each problem.  \textit{You will not get credit for work on unassigned pages (e.g., if you only selected the first page but your solution spans two pages, you would lose points for any part on the second page that the grader can't see).} 
\end{enumerate}

```{r, setup, include=FALSE}
library(tidyverse)
library(mdsr)
library(tidytext)
library(aRxiv)
library(wordcloud)

knitr::opts_chunk$set(
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
```

\newpage
# 1. Text as Data

NOTE: For this chapter, we'll be working with [Chapter 19 in the 2nd edition of the textbook](https://mdsr-book.github.io/mdsr2e/ch-text.html).

### a.
In Section 19.1.1, the `str_subset`, `str_detect`, and `str_which` functions are introduced for detecting a pattern in a character vector (like finding a needle in a haystack).  Explain what the 6 returned records indicate in each case of these three cases:

- `head(str_subset(macbeth, "  MACBETH"))`
- `head(str_which(macbeth, "  MACBETH"))`
- `head(str_detect(macbeth, "  MACBETH"))`

(Yes, the textbook explains the differences in these commands/calls to these commands, but it can be helpful if you run the lines yourself as well to be sure they work as you'd expect and to inspect the results.) 

> ANSWER: The 6 returned records from str_subset are the actual lines of the play the play which start with "  MACBETH". The ones returned from str_which are numbers corresponding to the indices of lines of the play that start with "  MACBETH". Finally, str_detect returned a bunch of FALSEs that tell us there are no lines containing "  MACBETH" within the first 6 elements of the dataset. This makes sense becayse str_which told us the first appearance of "  MACBETH" is the 228th element.

```{r}
# defining "macbeth" object
macbeth_url <- "http://www.gutenberg.org/cache/epub/1129/pg1129.txt"
Macbeth_raw <- RCurl::getURL(macbeth_url)
data(Macbeth_raw)
#Macbeth_raw

# strsplit returns a list: we only want the first element
macbeth <- stringr::str_split(Macbeth_raw, "\r\n")[[1]]
class(macbeth)
length(macbeth)

### finding literal strings
head(str_subset(macbeth, "  MACBETH"))
head(str_which(macbeth, "  MACBETH"))
head(str_detect(macbeth, "  MACBETH"))
```


### b.
Section 19.1.1 also introduces regular expressions.  Why do the two lines below differ in their results?

- `head(str_subset(macbeth, "MACBETH\\."))`
- `head(str_subset(macbeth, "MACBETH."))`

(Yes, the textbook explains the differences, but it can be helpful if you run the lines yourself as well to be sure they work as you'd expect and to inspect the results.) 

> ANSWER:  The first line of code is looking for the entire word MACBETH as it appears within the quotes and followed by a period. The second line of code uses the metacharacter function of . and is instead looking for any strings that contain the word MACBETH.

```{r}
head(str_subset(macbeth, "MACBETH\\."))
head(str_subset(macbeth, "MACBETH."))
```

### c. 

The following three commands look similar, but return different results.

- `head(str_subset(macbeth, "MAC[B-Z]"))`
- `head(str_subset(macbeth,"MAC[B|Z]"))`
- `head(str_subset(macbeth, "^MAC[B-Z]"))`

In words, explain what overall pattern is being searched for in each of the three cases (i.e., what do the patterns "MAC[B-Z]", "MAC[B|Z]", and "^MAC[B-Z]" indicate?)?

> ANSWER: 

```{r}
head(str_subset(macbeth, "MAC[B-Z]"))
head(str_subset(macbeth,"MAC[B|Z]"))
head(str_subset(macbeth, "^MAC[B-Z]"))

# (optional) other patterns to test out: 
head(str_subset(macbeth, "^MAC[B|Z]"))  # should return character(0) (nothing)
head(str_subset(macbeth, ".*MAC[B-Z]"))
head(str_subset(macbeth, ".MAC[B-Z]"))
head(str_subset(macbeth, "more$"))
```

### d.  OPTIONAL

In section 19.2.2, the `wordcloud` package is used to create a word cloud based on text in abstracts from Data Science articles in arXiv (which is "a fast-growing electronic repository of preprints of scientific papers from many disciplines").  I've provided some code below to get you started coding along with the example.  *This part (d) will not be graded, but is included to encourage you to test and explore some of the code in the extended example.*  What words are included in `tidytexts`'s `stop_words` dataset?  Do you think all of these words should be considered stop words (i.e. excluded from analysis) in all scenarios?  Are there any that might be useful in some contexts?  

```{r}
# note that the tidytext, aRxiv, and wordcloud packages were loaded in the 
# setup code chunk at the top of the program

# arxiv_search() is from the aRxiv package
DataSciencePapers <- arxiv_search(
  query = '"Data Science"', 
  limit = 20000, 
  batchsize = 100
)

glimpse(DataSciencePapers)

DataSciencePapers <- DataSciencePapers %>%
  mutate(
    submitted = lubridate::ymd_hms(submitted) 
    , updated = lubridate::ymd_hms(updated)
    , field = str_extract(primary_category, "^[a-z,-]+")
    , Field = ifelse(field == "cs", "Computer Science", "Other discipline"))

# stop words dataset provided by the tidytext package
stop_words <- tidytext::stop_words

# the unnest_tokens function is from the tidytext package
arxiv_words <- DataSciencePapers %>%
  unnest_tokens(output = word, input = abstract, token = "words") %>%
  anti_join(stop_words, by = "word") %>%
  select(word, id)

arxiv_word_freqs <- arxiv_words %>%
  count(id, word, sort = TRUE) %>%
  select(word, n, id)


# the wordcloud function is from the wordcloud package
# you may also need to install the "tm" package in order to use the function
set.seed(1962)
wordcloud(DataSciencePapers$abstract, 
  max.words = 40, 
  scale = c(8, 1), 
  colors = topo.colors(n = 30), 
  random.color = TRUE)
```

